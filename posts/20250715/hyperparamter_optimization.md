### **hyperparameter optimization (HPO)**

“调参”其实是深度学习工作中最常见、也是最有技术含量的部分之一。这里的“参”一般指 **超参数（hyperparameters）**，它们不是模型通过训练自动学习得到的参数，而是你在训练前需要人为设定的。

#### 在深度学习里，参数可以分两类：

- **模型参数（parameters）**：网络权重和偏置，是通过梯度下降等优化算法学出来的，不属于“调参”的范畴。
- **超参数（hyperparameters）**：需要人为指定的配置，直接影响训练过程和最终性能。所谓“调参”，就是在搜索和优化这些超参数。

#### 对于常见的超参数类别以及作用，按照功能来分：

1. 优化相关参数
   - **学习率 (learning rate)**：最核心的参数，决定每次更新权重的步长。 太大 → 不收敛或震荡；太小 → 收敛慢或停在局部最优。
   - **优化器 (optimizer)**：如 SGD、Adam、AdamW，不同优化器的动量机制、正则化方式不同。
   - **batch size**：影响梯度估计的噪声大小。小 batch → 泛化好但训练慢；大 batch → 训练快但可能陷入 sharp minima。
2. 模型结构相关参数
   - **网络深度/宽度**：层数和每层的神经元/通道数。深度过大容易过拟合或梯度消失。
   - **卷积核大小、注意力头数、隐藏层维度**等，直接影响模型表达能力和计算复杂度。
3. 正则化参数
   - **Dropout rate**：防止过拟合，但过高会削弱表达能力。
   - **L2 正则化系数 (weight decay)**：约束权重过大，帮助泛化。
   - **数据增强强度**：图像里的旋转、翻转，点云里的扰动等。
4. 训练策略参数
   - **学习率调度器 (scheduler)**：例如 Cosine Annealing、Step decay、Warmup。帮助模型稳定收敛。
   - **epoch 数量**：训练的迭代次数。
   - **早停 (early stopping)**：防止过拟合。

#### 如何去调整？

根据我的经验可以总结为一个层次化策略：

1. 先抓大头
   - 先把学习率和 batch size 调整好（一般对结果影响最大）。
   - 优化器选择也要合适，AdamW 在很多任务上是很稳健的。
2. 再调模型容量
   - 逐步加深或加宽模型，观察 validation 性能是否提升。
   - 避免盲目加大模型，否则会过拟合。
3. 正则化和数据增强
   - 如果发现训练集性能很好但验证集差 → 说明过拟合，需要增强正则化、数据增强。
4. 系统化搜索
   - Grid search / Random search：适合小规模任务。
   - Bayesian optimization / Hyperband / Population Based Training：适合大模型。
   - 实际科研中很多人用 **经验+少量搜索**，因为纯自动搜索成本高。





#### 一些经验性的黄金法则：

- 学习率调节是第一优先级，几乎所有问题 80% 都出在它上面。
- batch size 通常和学习率联动（大 batch 需要大 LR，小 batch 配合小 LR）。
- 不要一开始就把模型堆得很复杂，先用小模型快速验证 pipeline。
- 调参时只改一件事，保持对比清晰。
- 多观察 **训练曲线（loss/acc vs epoch）**，它比单个指标更能揭示问题。





#### 以下整理了一个 **深度学习调参优先级清单**，按实际经验和重要性排序。可以把它当作一个 checklist，每次实验时参考。

**一级：核心参数（必须优先调整）**

1. **学习率 (Learning Rate, LR)**
   - 影响最大。
   - 调整策略：先用较大 LR 试探，逐步减小，结合学习率调度器。
   - 常用范围：1e-1 ~ 1e-6，根据任务和优化器调整。

2. **batch size**
   - 与学习率联动。
   - 小 batch → 泛化更好但训练慢；大 batch → 训练快但可能过拟合或需调大学习率。
   - 常用范围：16–512（GPU 显存决定上限）。

3. **优化器 (Optimizer)**
   - AdamW、SGD+Momentum、RMSProp 常见。
   - 默认推荐 AdamW（稳定、泛化好），除非特定任务对 SGD 更敏感（如 CV 的经典 CNN）。

**二级：训练策略**

4. **学习率调度器 (Scheduler)**
   - Cosine Annealing / OneCycleLR / Step Decay。
   - 帮助模型从大步长探索到小步长收敛。
5. **训练 epoch 数量 & Early Stopping**
   - 太少 → 欠拟合；太多 → 过拟合。
   - 一般先训练较多 epoch，再用 Early Stopping 保证泛化。

**三级：模型容量**

6. **网络深度与宽度**
   - 逐步加大模型容量，观察验证集性能是否提升。
   - 切记：复杂模型 ≠ 好结果，先保证 pipeline 正确。
7. **隐藏层维度 / 注意力头数 / 卷积核大小**
   - 这些是精细结构超参数，需根据数据分布做调整。

**四级：正则化与泛化能力**

8. **Weight Decay (L2 正则化系数)**
   - 一般在 1e-5 ~ 1e-3。
   - 对 Transformer 类模型尤为重要。

9. **Dropout rate**
   - 一般 0.1 ~ 0.5。
   - 对小数据集防止过拟合有效，大数据集可以减弱甚至去掉。

**五级：进阶技巧**

10. **预训练权重 / 微调策略**
    - 用大模型预训练权重，再 fine-tune。
    - 冻结前几层 or 全部解冻，需实验。

11. **混合精度训练 (FP16)**
    - 加速收敛、节省显存，但需配合稳定的优化器。

12. **损失函数 (Loss Function)**
    - 分类：CrossEntropy 基础，Focal Loss 适合类别不平衡。
    - 回归：L1/L2 损失，SmoothL1。





#### 优先级顺序可以记成：

**核心训练参数（学习率 & batch size） > 学习率调度 & 训练时长 > 模型规模 > 正则化 > 进阶技巧**

这样能避免在次要细节上浪费时间，而是快速锁定对结果最敏感的因素。