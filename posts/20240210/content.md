### Transformer 架构代码手撕推导

论文：*Attention Is All You Need*

Transformer继承了Seq2Seq的“编码器-解码器”整体框架，它的任务是建立“输入序列特征”到“输出序列特征”的映射，但内部结构完全基于注意力机制和Feed-Forward网络，核心创新集中在**自注意力机制、多头注意力、位置编码、残差连接与层归一化**等模块，整体架构清晰且极具扩展性。

Transformer整体结构图如下：

<img src="ModalNet-Tansformer.png" style="zoom: 33%;" />

现在从模型的输入到最后的输出提供每一个部分的代码及其原理。

首先看模型的输入，在 `Input Embedding` 之前，模型最初接收的是离散的索引，`Inputs` 的形状是：`[batch_size, seq_len]`，其中 `batch_size` 是一次处理的样本数量，`seq_len` 是序列长度（例如一句话包含多少个单词/Token）。

我们得到输入后，数据开始向前传播，第一个要经过的是嵌入层，也就是 `Input Embedding` ，它的核心任务是将人类可读的离散 Token（词元）转化为机器可处理的、包含语义信息的稠密连续向量。

在自然语言处理中，原始数据是单词（如 "Apple", "King"），如果采用 One-hot 编码，向量维度会高达词表大小，且向量之间相互正交，这意味着在数学上，"Apple" 和 "Orange" 的距离与 "Apple" 和 "Car" 的距离是一样的，模型无法感知语义相关性。Embedding 将词汇映射到一个低维（如 512 维）的实数空间。在这个空间里，语义相近的词在几何距离上也会更接近。

虽然我们在神经网络中将其称为“层”，但 `nn.Embedding` 在本质上是一个巨大的查找表。假设词表大小（`src_vocab_size`）为 10,000，嵌入维度（`d_model`）为 512。那么这层维护着一个形状为 `[10000, 512]` 的权重矩阵 $W_{emb}$。当输入一个 Token 的索引 $i$ 时，它实际上是提取了矩阵 $W_{emb}$ 的第 $i$ 行。



